{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from semantic_kernel.agents import (\n",
    "    Agent,\n",
    "    ChatCompletionAgent,\n",
    "    MagenticOrchestration,\n",
    "    OpenAIAssistantAgent,\n",
    "    StandardMagenticManager,\n",
    ")\n",
    "from semantic_kernel.agents.runtime import InProcessRuntime\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAISettings\n",
    "from semantic_kernel.contents import ChatMessageContent\n",
    "\n",
    "\"\"\"\n",
    "The following sample demonstrates how to create a Magentic orchestration with two agents:\n",
    "- A Research agent that can perform web searches\n",
    "- A Coder agent that can run code using the code interpreter\n",
    "\n",
    "Read more about Magentic here:\n",
    "https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/\n",
    "\n",
    "This sample demonstrates the basic steps of creating and starting a runtime, creating\n",
    "a Magentic orchestration with two agents and a Magentic manager, invoking the\n",
    "orchestration, and finally waiting for the results.\n",
    "\n",
    "The Magentic manager requires a chat completion model that supports structured output.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def agents() -> list[Agent]:\n",
    "    \"\"\"Return a list of agents that will participate in the Magentic orchestration.\n",
    "\n",
    "    Feel free to add or remove agents.\n",
    "    \"\"\"\n",
    "    research_agent = ChatCompletionAgent(\n",
    "        name=\"ResearchAgent\",\n",
    "        description=\"A helpful assistant with access to web search. Ask it to perform web searches.\",\n",
    "        instructions=(\n",
    "            \"You are a Researcher. You find information without additional computation or quantitative analysis.\"\n",
    "        ),\n",
    "        # This agent requires the gpt-4o-search-preview model to perform web searches.\n",
    "        # Feel free to explore with other agents that support web search, for example,\n",
    "        # the `OpenAIResponseAgent` or `AzureAIAgent` with bing grounding.\n",
    "        service=OpenAIChatCompletion(ai_model_id=\"gpt-4o-search-preview\"),\n",
    "    )\n",
    "\n",
    "    # Create an OpenAI Assistant agent with code interpreter capability\n",
    "    client = OpenAIAssistantAgent.create_client()\n",
    "    code_interpreter_tool, code_interpreter_tool_resources = OpenAIAssistantAgent.configure_code_interpreter_tool()\n",
    "    definition = await client.beta.assistants.create(\n",
    "        model=OpenAISettings().chat_model_id,\n",
    "        name=\"CoderAgent\",\n",
    "        description=\"A helpful assistant that writes and executes code to process and analyze data.\",\n",
    "        instructions=\"You solve questions using code. Please provide detailed analysis and computation process.\",\n",
    "        tools=code_interpreter_tool,\n",
    "        tool_resources=code_interpreter_tool_resources,\n",
    "    )\n",
    "    coder_agent = OpenAIAssistantAgent(\n",
    "        client=client,\n",
    "        definition=definition,\n",
    "    )\n",
    "\n",
    "    return [research_agent, coder_agent]\n",
    "\n",
    "\n",
    "def agent_response_callback(message: ChatMessageContent) -> None:\n",
    "    \"\"\"Observer function to print the messages from the agents.\"\"\"\n",
    "    print(f\"**{message.name}**\\n{message.content}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the agents.\"\"\"\n",
    "    # 1. Create a Magentic orchestration with two agents and a Magentic manager\n",
    "    # Note, the Standard Magentic manager uses prompts that have been tuned very\n",
    "    # carefully but it accepts custom prompts for advanced users and scenarios.\n",
    "    # For even more advanced scenarios, you can subclass the MagenticManagerBase\n",
    "    # and implement your own manager logic.\n",
    "    # The standard manager also requires a chat completion model that supports\n",
    "    # structured output.\n",
    "    magentic_orchestration = MagenticOrchestration(\n",
    "        members=await agents(),\n",
    "        manager=StandardMagenticManager(chat_completion_service=OpenAIChatCompletion()),\n",
    "        agent_response_callback=agent_response_callback,\n",
    "    )\n",
    "\n",
    "    # 2. Create a runtime and start it\n",
    "    runtime = InProcessRuntime()\n",
    "    runtime.start()\n",
    "\n",
    "    # 3. Invoke the orchestration with a task and the runtime\n",
    "    orchestration_result = await magentic_orchestration.invoke(\n",
    "        task=(\n",
    "            \"I am preparing a report on the energy efficiency of different machine learning model architectures. \"\n",
    "            \"Compare the estimated training and inference energy consumption of ResNet-50, BERT-base, and GPT-2 \"\n",
    "            \"on standard datasets (e.g., ImageNet for ResNet, GLUE for BERT, WebText for GPT-2). \"\n",
    "            \"Then, estimate the CO2 emissions associated with each, assuming training on an Azure Standard_NC6s_v3 VM \"\n",
    "            \"for 24 hours. Provide tables for clarity, and recommend the most energy-efficient model \"\n",
    "            \"per task type (image classification, text classification, and text generation).\"\n",
    "        ),\n",
    "        runtime=runtime,\n",
    "    )\n",
    "\n",
    "    # 4. Wait for the results\n",
    "    value = await orchestration_result.get()\n",
    "\n",
    "    print(f\"\\nFinal result:\\n{value}\")\n",
    "\n",
    "    # 5. Stop the runtime when idle\n",
    "    await runtime.stop_when_idle()\n",
    "\n",
    "    \"\"\"\n",
    "    Sample output:\n",
    "    **ResearchAgent**\n",
    "    Estimating the energy consumption and associated CO₂ emissions for training and inference of ResNet-50, BERT-base...\n",
    "\n",
    "    **CoderAgent**\n",
    "    Here is the comparison of energy consumption and CO₂ emissions for each model (ResNet-50, BERT-base, and GPT-2)\n",
    "    over a 24-hour period:\n",
    "\n",
    "    | Model     | Training Energy (kWh) | Inference Energy (kWh) | Total Energy (kWh) | CO₂ Emissions (kg) |\n",
    "    |-----------|------------------------|------------------------|---------------------|---------------------|\n",
    "    | ResNet-50 | 21.11                  | 0.08232                | 21.19232            | 19.50               |\n",
    "    | BERT-base | 0.048                  | 0.23736                | 0.28536             | 0.26                |\n",
    "    | GPT-2     | 42.22                  | 0.35604                | 42.57604            | 39.17               |\n",
    "\n",
    "    ### Recommendations:\n",
    "    ...\n",
    "\n",
    "    **CoderAgent**\n",
    "    Here are the recalibrated results for energy consumption and CO₂ emissions, assuming a more conservative approach\n",
    "    for models like GPT-2:\n",
    "\n",
    "    | Model            | Training Energy (kWh) | Inference Energy (kWh) | Total Energy (kWh) | CO₂ Emissions (kg) |\n",
    "    |------------------|------------------------|------------------------|---------------------|---------------------|\n",
    "    | ResNet-50        | 21.11                  | 0.08232                | 21.19232            | 19.50               |\n",
    "    | BERT-base        | 0.048                  | 0.23736                | 0.28536             | 0.26                |\n",
    "    | GPT-2 (Adjusted) | 42.22                  | 0.35604                | 42.57604            | 39.17               |\n",
    "\n",
    "    ...\n",
    "\n",
    "    **ResearchAgent**\n",
    "    Estimating the energy consumption and associated CO₂ emissions for training and inference of machine learning ...\n",
    "\n",
    "    **ResearchAgent**\n",
    "    Estimating the energy consumption and CO₂ emissions of training and inference for ResNet-50, BERT-base, and ...\n",
    "\n",
    "    **CoderAgent**\n",
    "    Here is the estimated energy use and CO₂ emissions for a full day of operation for each model on an Azure ...\n",
    "\n",
    "    **ResearchAgent**\n",
    "    Recent analyses have highlighted the substantial energy consumption and carbon emissions associated with ...\n",
    "\n",
    "    **CoderAgent**\n",
    "    Here's the refined estimation for the energy use and CO₂ emissions for optimized models on an Azure ...\n",
    "\n",
    "    **CoderAgent**\n",
    "    To provide precise estimates for CO₂ emissions based on Azure's regional data centers' carbon intensity, we need ...\n",
    "\n",
    "    **ResearchAgent**\n",
    "    To refine the CO₂ emission estimates for training and inference of ResNet-50, BERT-base, and GPT-2 on an Azure ...\n",
    "\n",
    "    **CoderAgent**\n",
    "    Here's the refined comparative table for energy consumption and CO₂ emissions for ResNet-50, BERT-base, and GPT-2,\n",
    "    taking into account carbon intensity data for Azure's West Europe and Sweden Central regions:\n",
    "\n",
    "    | Model      | Energy (kWh) | CO₂ Emissions West Europe (kg) | CO₂ Emissions Sweden Central (kg) |\n",
    "    |------------|--------------|--------------------------------|-----------------------------------|\n",
    "    | ResNet-50  | 5.76         | 0.639                          | 0.086                            |\n",
    "    | BERT-base  | 9.18         | 1.019                          | 0.138                            |\n",
    "    | GPT-2      | 12.96        | 1.439                          | 0.194                            |\n",
    "\n",
    "    **Refined Recommendations:**\n",
    "\n",
    "    ...\n",
    "\n",
    "    Final result:\n",
    "    Here is the comprehensive report on energy efficiency and CO₂ emissions for ResNet-50, BERT-base, and GPT-2 models\n",
    "    when trained and inferred on an Azure Standard_NC6s_v3 VM for 24 hours.\n",
    "\n",
    "    ### Energy Consumption and CO₂ Emissions:\n",
    "\n",
    "    Based on refined analyses, here are the estimated energy consumption and CO₂ emissions for each model:\n",
    "\n",
    "    | Model      | Energy (kWh) | CO₂ Emissions West Europe (kg) | CO₂ Emissions Sweden Central (kg) |\n",
    "    |------------|--------------|--------------------------------|-----------------------------------|\n",
    "    | ResNet-50  | 5.76         | 0.639                          | 0.086                            |\n",
    "    | BERT-base  | 9.18         | 1.019                          | 0.138                            |\n",
    "    | GPT-2      | 12.96        | 1.439                          | 0.194                            |\n",
    "\n",
    "    ### Recommendations for Energy Efficiency:\n",
    "\n",
    "    ...\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f96046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ResearchAgent**\n",
      "To assess the energy efficiency of ResNet-50, BERT-base, and GPT-2 models during training and inference, we have compiled the following information:\n",
      "\n",
      "**1. Training Energy Consumption Estimates:**\n",
      "\n",
      "- **ResNet-50 on ImageNet:** Training ResNet-50 on the ImageNet dataset is estimated to consume approximately 0.9 MWh (900 kWh) of electricity. ([serp.ai](https://serp.ai/posts/total-energy/?utm_source=openai))\n",
      "\n",
      "- **BERT-base on GLUE:** Fine-tuning BERT-base on the GLUE benchmark varies by task. For instance, fine-tuning on the MNLI task consumes approximately 0.938 kWh on a single GPU. ([ar5iv.org](https://ar5iv.org/html/2311.10267?utm_source=openai))\n",
      "\n",
      "- **GPT-2 on WebText:** Training GPT-2, with 1.5 billion parameters, is estimated to consume significantly less energy than larger models like GPT-3, which consumes around 1,287 MWh. ([adasci.org](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/?utm_source=openai))\n",
      "\n",
      "**2. Inference Energy Consumption Estimates:**\n",
      "\n",
      "- **ResNet-50:** Inference energy consumption for ResNet-50 is relatively low, with estimates indicating that text classification tasks consume approximately 0.002 kWh per 1,000 queries. ([serp.ai](https://serp.ai/posts/total-energy/?utm_source=openai))\n",
      "\n",
      "- **BERT-base:** Inference for BERT-base is also energy-efficient, with similar consumption rates to ResNet-50 for text classification tasks. ([serp.ai](https://serp.ai/posts/total-energy/?utm_source=openai))\n",
      "\n",
      "- **GPT-2:** Inference for GPT-2 is more energy-intensive due to its larger size. For example, generating a 50-word response can consume between 0.001 to 0.02 kWh, depending on hardware and model size. ([linkedin.com](https://www.linkedin.com/pulse/ai-power-consumption-insights-manik-roy-enxfc?utm_source=openai))\n",
      "\n",
      "**3. Azure Standard_NC6s_v3 VM Specifications:**\n",
      "\n",
      "- **GPU Model:** The Azure Standard_NC6s_v3 virtual machine is equipped with an NVIDIA Tesla V100 GPU.\n",
      "\n",
      "- **Typical Power Draw:** The Tesla V100 GPU has a thermal design power (TDP) of approximately 250 watts.\n",
      "\n",
      "**4. CO₂ Emissions Factors:**\n",
      "\n",
      "- **Azure Data Centers:** Microsoft has committed to reducing carbon emissions, and their data centers aim to be carbon-neutral. However, specific CO₂ emissions factors can vary by region and energy source.\n",
      "\n",
      "- **General Emissions Factor:** In the United States, the average CO₂ emissions factor is approximately 0.954 pounds (0.433 kg) of CO₂ per kWh consumed. ([docslib.org](https://docslib.org/doc/12559028/energy-and-policy-considerations-for-deep-learning-in-nlp?utm_source=openai))\n",
      "\n",
      "**5. Typical Training Durations:**\n",
      "\n",
      "- **ResNet-50 on ImageNet:** Training ResNet-50 on ImageNet can take several days, depending on hardware and batch size.\n",
      "\n",
      "- **BERT-base on GLUE:** Fine-tuning BERT-base on GLUE tasks can range from minutes to several hours, depending on the specific task and hardware used.\n",
      "\n",
      "- **GPT-2 on WebText:** Training GPT-2 from scratch on the WebText dataset can take several weeks on multiple GPUs.\n",
      "\n",
      "**Calculations for 24-Hour Training on Azure Standard_NC6s_v3 VM:**\n",
      "\n",
      "- **Energy Consumption:** Assuming the Tesla V100 GPU operates at full capacity (250 watts) for 24 hours:\n",
      "\n",
      "  250 watts * 24 hours = 6,000 watt-hours = 6 kWh\n",
      "\n",
      "- **CO₂ Emissions:** Using the U.S. average emissions factor:\n",
      "\n",
      "  6 kWh * 0.433 kg CO₂/kWh = 2.598 kg CO₂\n",
      "\n",
      "**Summary Table:**\n",
      "\n",
      "| Model       | Training Energy Consumption | Inference Energy Consumption | Training Duration | 24-Hour Training Energy | 24-Hour Training CO₂ Emissions |\n",
      "|-------------|-----------------------------|------------------------------|-------------------|-------------------------|---------------------------------|\n",
      "| ResNet-50   | ~900 kWh                    | ~0.002 kWh/1,000 queries     | Several days      | 6 kWh                   | 2.598 kg CO₂                   |\n",
      "| BERT-base   | Varies by task              | Similar to ResNet-50         | Minutes to hours  | 6 kWh                   | 2.598 kg CO₂                   |\n",
      "| GPT-2       | Significantly less than GPT-3 | 0.001-0.02 kWh per 50-word prompt | Several weeks     | 6 kWh                   | 2.598 kg CO₂                   |\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "- **Image Classification:** ResNet-50 is the most energy-efficient model for image classification tasks.\n",
      "\n",
      "- **Text Classification:** BERT-base is suitable for text classification tasks, offering a balance between performance and energy consumption.\n",
      "\n",
      "- **Text Generation:** While GPT-2 is more energy-intensive, it is appropriate for text generation tasks. However, consider the energy implications, especially during inference.\n",
      "\n",
      "*Note: The above estimates are approximate and can vary based on specific hardware configurations, optimization techniques, and operational conditions.* \n",
      "**CoderAgent**\n",
      "# Define parameters\n",
      "hours_of_training = 24\n",
      "gpu_power_watts = 250\n",
      "energy_per_24h = (gpu_power_watts * hours_of_training) / 1000  # kWh\n",
      "co2_per_kwh = 0.433\n",
      "\n",
      "# Models and tasks\n",
      "model_data = [\n",
      "    # Model, Task type, Estimate_inference_kwh_per_1000, Typical_training_footprint_kwh (for reference)\n",
      "    {\n",
      "        \"Model\": \"ResNet-50\",\n",
      "        \"Task\": \"Image classification\",\n",
      "        \"Inference_kWh_per_1000\": 0.002,  # estimate found\n",
      "        \"Full_training_footprint_kWh\": 900  # Estimated full train (cited above)\n",
      "    },\n",
      "    {\n",
      "        \"Model\": \"BERT-base\",\n",
      "        \"Task\": \"Text Classification\",\n",
      "        \"Inference_kWh_per_1000\": 0.002,  # assumed similar to ResNet-50 (text tasks)\n",
      "        \"Full_training_footprint_kWh\": 50  # Fine-tuning on GLUE much less than pretraining; rough median estimate (varies widely)\n",
      "    },\n",
      "    {\n",
      "        \"Model\": \"GPT-2\",\n",
      "        \"Task\": \"Text Generation\",\n",
      "        \"Inference_kWh_per_1000\": 0.01,  # higher estimate\n",
      "        \"Full_training_footprint_kWh\": 2000  # Very rough order-of-magnitude estimate (GPT-2 is much less than GPT-3, but still high)\n",
      "    },\n",
      "]\n",
      "\n",
      "# Calculate 24h training energy/CO2 and inference energy/CO2 per 1000 queries\n",
      "summary_rows = []\n",
      "recommendations = {}\n",
      "\n",
      "for entry in model_data:\n",
      "    train_energy = energy_per_24h\n",
      "    train_co2 = train_energy * co2_per_kwh\n",
      "\n",
      "    inf_energy = entry[\"Inference_kWh_per_1000\"]\n",
      "    inf_co2 = inf_energy * co2_per_kwh\n",
      "\n",
      "    summary_rows.append({\n",
      "        \"Model\": entry[\"Model\"],\n",
      "        \"Task Type\": entry[\"Task\"],\n",
      "        \"24h Training Energy (kWh)\": train_energy,\n",
      "        \"24h Training CO2 (kg)\": train_co2,\n",
      "        \"Inference per 1000 Queries Energy (kWh)\": inf_energy,\n",
      "        \"Inference per 1000 Queries CO2 (kg)\": inf_co2,\n",
      "    })\n",
      "\n",
      "    # For recommendation: Lowest inference energy for task\n",
      "    task = entry[\"Task\"]\n",
      "    if task not in recommendations or inf_energy < recommendations[task][\"inf_energy\"]:\n",
      "        recommendations[task] = {\"model\": entry[\"Model\"], \"inf_energy\": inf_energy}\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "summary_table = pd.DataFrame(summary_rows)\n",
      "\n",
      "# Recommendations table\n",
      "rec_table = pd.DataFrame([\n",
      "    {\"Task Type\": k, \"Most Energy-Efficient Model\": v[\"model\"], \"Inference Energy per 1000 (kWh)\": v[\"inf_energy\"]}\n",
      "    for k, v in recommendations.items()\n",
      "])\n",
      "\n",
      "summary_table, rec_table### (a) Estimated Training Energy and CO₂ Emissions (24 Hours, Azure Standard_NC6s_v3)\n",
      "\n",
      "| Model      | Task Type            | 24h Training Energy (kWh) | 24h Training CO₂ (kg) |\n",
      "|------------|----------------------|---------------------------|-----------------------|\n",
      "| ResNet-50  | Image classification |         6.0               |       2.598           |\n",
      "| BERT-base  | Text Classification  |         6.0               |       2.598           |\n",
      "| GPT-2      | Text Generation      |         6.0               |       2.598           |\n",
      "\n",
      "---\n",
      "### (b) Inference Energy and CO₂ Emission per 1,000 Queries\n",
      "\n",
      "| Model      | Task Type            | Inference per 1,000 Queries Energy (kWh) | Inference per 1,000 Queries CO₂ (kg) |\n",
      "|------------|----------------------|------------------------------------------|--------------------------------------|\n",
      "| ResNet-50  | Image classification | 0.002                                    |   0.00087                            |\n",
      "| BERT-base  | Text Classification  | 0.002                                    |   0.00087                            |\n",
      "| GPT-2      | Text Generation      | 0.010                                    |   0.00433                            |\n",
      "\n",
      "---\n",
      "### (c) Most Energy-Efficient Model Per Task Type\n",
      "\n",
      "| Task Type            | Most Energy-Efficient Model | Inference Energy per 1,000 (kWh) |\n",
      "|----------------------|----------------------------|----------------------------------|\n",
      "| Image classification | ResNet-50                  |             0.002                |\n",
      "| Text Classification  | BERT-base                  |             0.002                |\n",
      "| Text Generation      | GPT-2                      |             0.010                |\n",
      "\n",
      "---\n",
      "\n",
      "## Key Assumptions and Notes\n",
      "\n",
      "- **Hardware**: All models are assumed to train on an Azure NC6s_v3 VM (NVIDIA V100 GPU, 250W TDP) for exactly 24 hours of full utilization (6 kWh total energy drawn from GPU alone; actual system draw may be higher when including CPUs, RAM, etc.).\n",
      "- **CO₂ Emissions Factor**: Used 0.433 kgCO₂/kWh (US average), which may differ regionally or for carbon-neutral datacenters.\n",
      "- **Inference Energy**: Values are typical/approximate for 1,000 forward queries; actual numbers depend on hardware, batch size, sequence/image size, etc.\n",
      "- **Training Duration**: 24 hours is usually insufficient for full converged training (especially for GPT-2); this is a baseline for hardware comparison.\n",
      "- **Other Energy Costs**: These results do not include storage, network, or other system-level energy contributions.\n",
      "- **Model Sizes**: Energy for full pretraining (especially for GPT-2) is much higher than shown here; this estimate is for the specified single-node, 24h scenario only.\n",
      "\n",
      "---\n",
      "**In summary:**  \n",
      "- For image classification, ResNet-50 is both performant and energy-efficient.\n",
      "- For text classification, BERT-base is the practical and energy-conscious choice.\n",
      "- For text generation, GPT-2 is currently the most efficient among large generators, but energy costs are highest for this use-case.\n",
      "\n",
      "If you need full-training or multi-GPU estimates, or want to include more system costs, let me know!\n",
      "\n",
      "Final result:\n",
      "Certainly! Here is a clear comparison of the estimated energy efficiency and CO₂ emissions for ResNet-50, BERT-base, and GPT-2, based on 24 hours of training on an Azure Standard_NC6s_v3 VM (NVIDIA V100 GPU, 250W TDP, standard emissions factor), along with a summary of inference costs and recommendations:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Estimated Training Energy and CO₂ Emissions (24-hour run on Azure Standard_NC6s_v3)\n",
      "\n",
      "| Model      | Task Type            | 24h Training Energy (kWh) | 24h Training CO₂ Emissions (kg) |\n",
      "|------------|----------------------|---------------------------|----------------------------------|\n",
      "| ResNet-50  | Image classification |         6.0               |       2.60                       |\n",
      "| BERT-base  | Text classification  |         6.0               |       2.60                       |\n",
      "| GPT-2      | Text generation      |         6.0               |       2.60                       |\n",
      "\n",
      "*Note: This assumes full utilization of the GPU only; actual full-node draw can be higher. Real-world training (especially for full GPT-2 pretraining) usually takes much longer.*\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Inference Energy and CO₂ Emissions per 1,000 Queries\n",
      "\n",
      "| Model      | Task Type            | Inference Energy (kWh / 1,000 queries) | Inference CO₂ (kg / 1,000 queries) |\n",
      "|------------|----------------------|----------------------------------------|-------------------------------------|\n",
      "| ResNet-50  | Image classification | 0.002                                  | 0.00087                             |\n",
      "| BERT-base  | Text classification  | 0.002                                  | 0.00087                             |\n",
      "| GPT-2      | Text generation      | 0.010                                  | 0.00433                             |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Recommendations for Most Energy-Efficient Model per Task\n",
      "\n",
      "| Task Type            | Most Energy-Efficient Model | Inference Energy per 1,000 (kWh) |\n",
      "|----------------------|----------------------------|----------------------------------|\n",
      "| Image classification | ResNet-50                  | 0.002                            |\n",
      "| Text classification  | BERT-base                  | 0.002                            |\n",
      "| Text generation      | GPT-2                      | 0.010                            |\n",
      "\n",
      "---\n",
      "\n",
      "#### Notes & Assumptions\n",
      "\n",
      "- **Hardware**: Based on GPU TDP (250W); does not include full-system overhead.\n",
      "- **CO₂ Factor**: Calculated using 0.433 kg CO₂ per kWh (average US grid emissions).\n",
      "- **Inference**: Estimates are typical (batch and hardware can affect actual numbers); GPT-2 is more energy-intensive for each text generation than BERT or ResNet-50 for their tasks.\n",
      "- **Training**: 24 hours is less than full convergence for ImageNet or GPT-2, but used here for consistent hardware comparisons.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary & Recommendations**\n",
      "\n",
      "- For **image classification**, **ResNet-50** is the most energy-efficient standard architecture.\n",
      "- For **text classification**, **BERT-base** provides high accuracy with efficiency.\n",
      "- For **text generation**, **GPT-2** is more energy-intensive than the other models, but remains the efficient default among large-scale generative models.\n",
      "\n",
      "If you want more granular estimates (full training cost, inference on different hardware, or including full system power), let me know!\n",
      "\n",
      "**References & Sources**:\n",
      "- [Azure NC6s_v3 VM specs](https://learn.microsoft.com/en-us/azure/virtual-machines/ncv3-series)\n",
      "- [US DOE Emissions Factor](https://docslib.org/doc/12559028/energy-and-policy-considerations-for-deep-learning-in-nlp)\n",
      "- [Estimates: serp.ai, ar5iv.org, Adasci.org, LinkedIn AI power consumption analysis]\n",
      "\n",
      "Let me know if you need any further detail, breakdowns, or visualizations.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
